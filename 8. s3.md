	1. Object Storage
	2. Main building blocks of AWS advertised as "infinitely scaling" storage.
	3. Many websites uses s3 and it integrates with many AWS services.
	4. Pay for storage
	5. You can Either use this service directly or indirectly by using other AWS services which uses S3 as backbone ( RDS and EBS snapshots are stored in S3)
	6. Use cases:
		a. Backup for on premise data
		b. Blob storage data with index that data in another service like DynamoDB

Buckets:
	1. Allows you to store objects in buckets (files in directories).
	2. High durability with 99.99999999999 % durability across multiple AZ that sustains two concurrent loss of AZ.
	3. It’s a global service but buckets are created in region and defined in regional level.
	4. It must have globally unique name with naming conventions
	5. No uppercase, No underscore
		a. 3-63 characters long
		b. Not an IP.
		c. Must start with lowercase letter or number
		d. Must NOT start with the prefix "xn--"
		e. Must NOT end with the suffix -s3alias 
		
Objects:
	1. Objects(files) have a key.
	2. Object contains  both data and metadata.
	3. The key is full path composed of prefix + object name.
	4. There is no concept of directories with in buckets just keys with long names contains slashes ("/")
		a. s3://my-bucket/my_folder1/another_folder/my_file.txt
	5. Object URL contains bucket name and key:
		a. Eg: http://mybucket.s3.amazonaws.com/filename.txt
	6. Objects Max size is 5Tb and use multi-part upload if it is >5GB
	7. Metadata (list of text key / value pairs – system or user metadata)
	8. System metadata like file created is managed by s3
	9. Custom Metadata: Tags (Unicode key / value pair – up to 10) – useful for security / lifecycle
	10. Version ID (if versioning is enabled).
	11. Even though bucket is owned by account A. if Account B uploaded some objects to that bucket. Owner of bucket wont have implicit access. 

Amazon S3 Operations:
	1. Create/Delete bucket
	2. Write/Read/Delete an object
	3. List keys in a bucket
	4. S3 uses REST API and maps HTTP verbs to CRUD operations
		a. HTTP PUT - create
		b. HTTP GET - read
		c. HTTP POST - update
		d. HTTP DELETE -delete
	5. Always use https.
	6. Most cases users don’t use REST interface directly but instead they interact with SDK, CLI or console which uses REST APIs
	7. Read after write consistency - returns stale data until object update ( you might get a new data or old data but not mix of both)
	
Security:
	1. User Based - IAM policies should allow for specific user 
	2. Resource Based
		a. Bucket Policies - bucket wide rules from s3 console.( allows cross account).
		b. Object Access Control list (ACL)- fine grain (can be disabled)
		c. Bucket Access Control list (ACL)-less common (can be disabled)
	3. Encryption: encrypt object in s3 using encryption keys.
	
Bucket Policies
	1. JSON based policies
		a. Resources: bucket and objects.
		b. Effect: Allow/Deny
		c. Actions: set of API to Allow or Deny.
		d. Principal: the account or user to apply the policy to.
	2. Use s3 bucket for policy to:
		a. Grant public access to bucket.
		b. Force objects to encrypt at upload.
		c. Grant access to another account 
	3. Bucket should never be public
	
Static Web Hosting:
	1. S3 can host static websites and have them accessible on the internet.
	2. The website URL will be depending on the region.
	3. If you get a 403 Forbidden error, make sure the bucket policy allows public reads.
	
Versioning:
	1. Enables at bucket level.
	2. Version will overwrite every time object updates.
	3. Prior version before enabling version is null and suspended versioning will not delete previous versions.
	4. Only service you can suspend but cant disable ( means going back to un versioned bucket)
	
Replication:
	1. Must enable versioning in both the buckets.
	2. Metadata and ACLs associated with the object are also replicated.
	3. Cross region and Same region replication can be done.
	4. Buckets can be in different AWS Accounts.
	5. Copying is asynchronous 
	6. Must have proper IAM permissions to s3
	7. All new objects will be replicated. You need to use s3 batch replication for older objects.
	8. Can replicate delete markers and deletions with version id are not replicated.
	9. No chain replication (A -> B -> C)
	
Storage Classes:
	1. General Purpose:
		a. For frequently accessed data with low latency and high throughput. Sustains 2 concurrent facility failures.
	2. Standard Infrequent Access (IA):
		a. For Less frequently accessed data, lower cost
		b. Minimum storage cost for 30 days.
		c. 128Kb min object size.
	3. One Zone Infrequent Access:
		a. Strong secondary backup with low cost.
		b. Minimum storage cost for 30 days.
		c. Reduce Redundancy storage.
	4. Glacier Instant Retrieval:
		a. Milliseconds retrieval, for data accessed once a quarter.
		b. Minimum storage 90 days.
		c. Issue a restore command using Amazon S3 APIs to retrieve data that copies data to S3 RRS (copy)
		d. 5% data retrieval free each month.
		e. Incur restore fee.
	5. Glacier Flexible Retrieval:
		a. Expedited ( 1 to 5 mins), standard (3to 5 hours) bulk(5 to 12 hours)
		b. Free and min 90 days storage
	6. Glacier Deep archive:
		a. Standard(12 hours), bulk (48 hours)
		b. Min storage 180 days
	7. Intelligent tiering:
		a. Moves objects automatically between access tiers based on usage.
		b. There are no retrieval charges.


Life cycle rules:
	1. Moving objects between storage classes can be automated using life cycle rules.
	2. You can apply to all objects in bucket or specified by a prefix.
	3. Transition Actions - configure objects to transition to another storage class.
		a. Eg.  Move object to standard IA after 60 days or Move to glacier archive after 60 days.
	4. Expiry Actions - configure objects to delete after some time.
		a. Access log files can be set to delete after 365 days.
		b. Can be used to delete old files if version is enabled or delete incomplete multi-part upload files.
		c. Rules can be created using prefix or tags.

S3 Analytics:
	1. Helps you to decide transition objects to right storage class.
	2. Report is updated daily.
	3. 24 hr to 48 hours to start seeing data analysis.

S3 Requester Pays:
	1. In general, bucket owner pay for all s3 storage and data transfer costs associated with the bucket.
	2. With requester pays bucket, the requester instead of bucket owner pays the cost of the request and the data download from the bucket.
	3. Helpful when you want to share large datasets with other accounts.
	4. The requester must be authenticated.
	
Event notifications:
	1. Event notification typically deliver events in seconds but can sometimes take a minute or longer
	2. Name filtering is possible.
	3. You can send events through SNS, SQS or directly to lambda.
	4. Event Bridge.
		a. Advanced filtering options with JSON rules. (metadata, object size etc)
		b. Multiple destinations - step functions, kinesis streams etc.
		c. Event bridge capabilities - Archive, replay events, reliable deliver.
	
S3 Transfer acceleration:
	1. Increase transfer speed by transferring file to an AWS edge location which will forward the data to the s3 bucket in the target region.
	2. Compatible with multi-part upload.

S3 Byte-Range Fetches:
	1. Parallelize GETs by requesting specific byte ranges.
	2. Better resilience in case of failures.
	
S3 Select & Glacier Select:
	1. Retrieve less data using SQL by performing server side filtering.
	2. Can filter by rows & columns
	3. Less network transfer and less CPU cost client side.
	
S3 Batch operations:
	1. Perform bulk operations with single request.
	2. Eg. 
		a. Modify object metadata and properties.
		b. Copy objects between s3 Buckets.
	3. Job consists of list of objects and action with optional parameters.
	4. Manages retires, tracks progress send completion notifications and generate reports.
	5. You can use s3 inventory to get object list and use s3 select to filter your objects.

Object Encryption:
	1. You can encrypt object in one of 4 methods provided below:
		a. Server Side Encryption (SSE)
		b. With AWS S3 managed keys (SSE-S3)
			i. Encryption type AES256
			ii. Set headers "x-amz-server-side-encryption": "AES256"
			iii. Keys managed by AWS S3 in secure hosts and further enhancing protection.
		c. KMS Keys stored in AWS KMS. (SSE-KMS)
			i. Set header ""x-amz-server-side-encryption": "aws:kms"
			ii. Limitation - while downloading or uploading it uses KMS API.
			iii. AWS KMS Service handles the keys.
			iv. Helps in auditing.
		d. Customer Provided Keys (SSE-C):
			i. Doesn't store the encryption key you provided.
			ii. Encryption key must provided in http headers.
			iii. Encryption will be done on AWS side using the key provided.
		e. Client side Encryption.
			i. Use client side encryption library.
			ii. Customer should manage encryption and decryption.
	2. Encryption in flight also called SSL/TLS.
	3. S3 exposes http and https endpoints.
	4. HTTPS is recommended and mandatory for SSE-C 
	5. You can force encryption using bucket policies.

CORS:
	1. Cross origin resource sharing.
	2. Origin  = scheme(protocol) + host (domain) + port
	3. Web browser based mechanism to allow requests to other origins while visiting the main origin.
	4. Request wont be fulfilled unless the other origin allows for the requests. using CORS headers (example Access-Control-Allow-Origin)
	5. You can allow * for all regions.
	6. If client makes CORS requests on our s3 buckets, we need to enable the correct CORS headers.

MFA Delete:
	1. You can enable MFA to delete a object in the bucket.
	2. MFA is required for permanently delete an object or suspend versioning.
	3. Won't required to enable versioning and list deleted versions.
	4. Only bucket owner(root account) can enable / disable MFA delete
	
Access logs:
	1. For audit purpose, you may want to log all access to s3 buckets.
	2. Any request made to s3 bucket from any account, authorized or denied will be logged into another s3 bucket.
	3. The data can be analysed using data analysis tools.
	4. The target logging bucket must be in same region.
	5. Don’t set your logging bucket to be monitored bucket. It will create al logging loop and you bucket will grow exponentially.

Pre-signed URLs:
	1. You can generate pre signed URLs using s3 console, CLI or SDK..
	2. URL Expiration
		a. S3 console = 1min to 720 mins(12 hours)
		b. AWS CLI - default: 3600secs (1hour) max- 604800 secs (168hours, 1 Week).
	3. Inherits the permissions generated by the user.
	4. Helps in protect against "Content Scraping" of web content such as media files stored in S3

Multipart Upload:
	1. To upload large objects to Amazon S3. S3 provides multi-part upload API. (through parallel transfers)
	2. Ability to pause and resume uploads if size is unknown.
	3. 3 step process ( initialization, uploading parts and completion or abort).
	4. Parts uploaded and assembles to create object
	5. Use for files >100MB
	6. Aws s3 cp uses multipart upload for large objects.
	7. Use life cycle policy to delete incomplete uploads after x days

S3 Object lock:
	1. Adopt a WORM (Write Once read many) model.
	2. Block an object version deletion for a specified amount of time.
	3. Retention mode - Compliance:
		a. Versions can't be overwritten or deleted by user including root user.
		b. Retention mode can't be changed or period can't be shortened.
	4. Retention mode - Governance:
		a. Most users won't have permissions to delete or change settings.
		b. Some users have special permissions to change the retention or delete the object
	5. Retention period: protect the object for a fixed period, it can be extended.
	6. Legal hold: 
		a. Protect the object indefinitely, independent from retention period.
		b. Can be freely placed and removed using the s3:PutObjectLegalHold IAM permission.

Access Points:
	1. Each access point gets its own DNS and policy to limit who can access it 
	2. A specific IAM user/ group.
	3. One policy per access point. => easier to manage than complex bucket policies.

S3 Object Lambda:
	1. Use AWS Lambda functions to change the object before it is retrieved by the caller application.
	2. Only one s3 bucket is needed, on top of which we create s3 access point and s3 object lambda access points.
	3. Use cases: 
		a. converting xml files to json files.
		b. Adding watermarking to images.
	
Glacier:
	1. Stores archives. Each archive ( 40TB of data)
	2. Each archive has unique ID (assigned at the time of creation)
	3. You cannot specify user friendly archive name. 
	4. All archives are automatically encrypted and immutable.

Vault:
	1. Containers for archives. Upto 1000 vaults for AWS Accounts.
	2. You can control access to vaults using IAM policies or vault access policies.
	
Glacier Vault Lock:
	1. Adopt a WORM (Write once read many) model.
	2. Create a vault lock policy.
	3. Lock the policy for future edits
	4. Helpful for compliance and data retention
